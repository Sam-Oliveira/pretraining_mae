## Impact of the pre-training data distribution on the fine-tuned performance of MAEs

This repo contains the code developed to study the impact of different pre-training data distributions on the downstream fine-tuned performance of Masked Autoencoders. The report can be accessed [here](https://drive.google.com/file/d/1xEna-rL-4xCyeZKJZ5UoOvzY6QIUK-oB/view?usp=sharing).


### How to run

The "instruction.pdf" file contains detailed instructions on how to reproduce the results contained in the report. 
